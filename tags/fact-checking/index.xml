<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>fact-checking on CopeNLU</title>
    <link>https://copenlu.github.io/tags/fact-checking/</link>
    <description>Recent content in fact-checking on CopeNLU</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Nov 2025 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://copenlu.github.io/tags/fact-checking/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>8 Papers Accepted to EMNLP 2025</title>
      <link>https://copenlu.github.io/news/8-papers-accepted-to-emnlp-2025/</link>
      <pubDate>Tue, 04 Nov 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/8-papers-accepted-to-emnlp-2025/</guid>
      <description>8 papers by CopeNLU authors are accepted to appear at EMNLP 2025, on topics including explainability and cross-cultural NLP.
Graph-Guided Textual Explanation Generation Framework. Shuzhou Yuan, Jingyi Sun, Michael Färber, Steffen Eger, Pepa Atanasova, Isabelle Augenstein.
Self-Critique and Refinement for Faithful Natural Language Explanations. Yingming Wang, Pepa Atanasova.
FLARE: Faithful Logic-Aided Reasoning and Exploration. Erik Arakelyan, Pasquale Minervini, Pat Verga, Patrick Lewis, Isabelle Augenstein.
Explainability and Interpretability of Multilingual Large Language Models: A Survey.</description>
    </item>
    
    <item>
      <title>3 Papers to be Presented at ACL 2025</title>
      <link>https://copenlu.github.io/news/3-papers-to-be-presented-at-acl-2025/</link>
      <pubDate>Fri, 16 May 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/3-papers-to-be-presented-at-acl-2025/</guid>
      <description>3 papers by CopeNLU authors are accepted to be presented ACL 2025, on topics including fact checking, retrieval-augmented generation and cultural NLP.
Can Community Notes Replace Professional Fact-Checkers?. Greta Warren, Nadav Borenstein, Desmond Elliott, Isabelle Augenstein.
A Reality Check on Context Utilisation for Retrieval-Augmented Generation. Lovisa Hagström, Sara Vera Marjanović, Haeun Yu, Arnav Arora, Christina Lioma, Maria Maistro, Pepa Atanasova, Isabelle Augenstein.
Survey of Cultural Awareness in Language Models: Text and Beyond.</description>
    </item>
    
    <item>
      <title>4 Papers Accepted to NAACL 2025</title>
      <link>https://copenlu.github.io/news/4-papers-accepted-to-naacl-2025/</link>
      <pubDate>Wed, 01 Jan 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/4-papers-accepted-to-naacl-2025/</guid>
      <description>4 papers by CopeNLU authors are accepted to appear at NAACL 2025, on topics including interpretability and computational social science.
A Unified Framework for Input Feature Attribution Analysis. Jingyi Sun, Pepa Atanasova, Isabelle Augenstein.
Investigating Human Values in Online Communities. Nadav Borenstein, Arnav Arora, Lucie-Aimée Kaffee , Isabelle Augenstein.
Specializing Large Language Models to Simulate Survey Response Distributions for Global Populations. Yong Cao, Arnav Arora, Isabelle Augenstein, Paul Röttger.
Measuring and Benchmarking Large Language Models&amp;rsquo; Capabilities to Generate Persuasive Language.</description>
    </item>
    
    <item>
      <title>5 Papers Accepted to EMNLP 2024</title>
      <link>https://copenlu.github.io/news/5-papers-accepted-to-emnlp-2024/</link>
      <pubDate>Thu, 07 Nov 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/5-papers-accepted-to-emnlp-2024/</guid>
      <description>5 papers by CopeNLU authors are accepted to appear at EMNLP 2024, on topics including factuality and probing for bias.
Social Bias Probing: Fairness Benchmarking for Language Models. Marta Marchiori Manerba, Karolina Stańczak, Riccardo Guidotti, Isabelle Augenstein.
Can Transformers Learn n-gram Language Models?. Anej Svete, Nadav Borenstein, Mike Zhou, Isabelle Augenstein, Ryan Cotterell.
DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models. Sara Vera Marjanović, Haeun Yu, Pepa Atanasova, Maria Maistro, Maria Maistro, Christina Lioma, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>5 Papers Accepted to EMNLP 2023</title>
      <link>https://copenlu.github.io/news/5-papers-accepted-to-emnlp-2023/</link>
      <pubDate>Tue, 05 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/5-papers-accepted-to-emnlp-2023/</guid>
      <description>5 papers by CopeNLU authors are accepted to appear at EMNLP 2023, on topics ranging from explainability to language modelling.
Explaining Interactions Between Text Spans. Sagnik Ray Choudhury, Pepa Atanasova, Isabelle Augenstein.
Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions. Lucie-Aimée Kaffee, Arnav Arora, Isabelle Augenstein.
Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing. Lucie-Aimée Kaffee, Arnav Arora, Zeerak Talat, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>4 Papers Accepted to ACL 2023</title>
      <link>https://copenlu.github.io/news/4-papers-accepted-to-acl-2023/</link>
      <pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/4-papers-accepted-to-acl-2023/</guid>
      <description>4 papers by CopeNLU authors are accepted to appear at ACL 2023. The papers make contributions within faithfulness of explanations, measuring intersectional biases, event extraction and few-shot stance detection.
Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection. Erik Arakelyan, Arnav Arora, Isabelle Augenstein.
Faithfulness Tests for Natural Language Explanations. Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, Isabelle Augenstein.
Measuring Intersectional Biases in Historical Documents. Nadav Borenstein, Karolina Stańczak, Thea Rolskov, Natacha Klein Käfer, Natália da Silva Perez, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>2 Papers Accepted to EMNLP 2022</title>
      <link>https://copenlu.github.io/news/2-papers-accepted-to-emnlp-2022/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/2-papers-accepted-to-emnlp-2022/</guid>
      <description>2 papers by CopeNLU authors are accepted to appear at EMNLP 2022, which are on scholarly document understanding.
Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection. Indira Sen, Mattia Samory, Claudia Wagner, Isabelle Augenstein.
Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings. Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein, Bela Gipp, Georg Rehm.</description>
    </item>
    
    <item>
      <title>3 Papers Accepted to NAACL 2022</title>
      <link>https://copenlu.github.io/news/3-papers-accepted-to-naacl-2022/</link>
      <pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/3-papers-accepted-to-naacl-2022/</guid>
      <description>3 papers by CopeNLU authors are accepted to appear at NAACL 2022, which are on the topics of hatespeech detection, misinformation detection and multilingual probing.
Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection. Indira Sen, Mattia Samory, Claudia Wagner, Isabelle Augenstein.
A Survey on Stance Detection for Mis- and Disinformation Identification. Momchil Hardalov, Arnav Arora, Preslav Nakov, Isabelle Augenstein.
Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models.</description>
    </item>
    
    <item>
      <title>2 Papers Accepted to AAAI 2022</title>
      <link>https://copenlu.github.io/news/2-papers-accepted-to-aaai-2022/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/2-papers-accepted-to-aaai-2022/</guid>
      <description>2 papers by CopeNLU authors are accepted to appear at AAAI 2022. One paper is on explanation generation, demonstrating how directly optimising for diagnostic properties of explanations, such as faithfulness, data consistency and confidence indication, can improve explanation quality. The other paper presents the most comprehensive study of cross-lingual stance detection to date, and proposes methods for learning with limited labelled data across languages and domains.
Diagnostics-Guided Explanation Generation. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>Paper Accepted to IJCAI 2021</title>
      <link>https://copenlu.github.io/news/paper-accepted-to-ijcai-2021/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/paper-accepted-to-ijcai-2021/</guid>
      <description>A paper by CopeNLU author is accepted to appear at IJCAI 2021. The paper studies how to perform complex claim verification on naturally occurring political claims with multiple hops over evidence chunks.
Multi-Hop Fact Checking of Political Claims. Wojciech Ostrowski, Arnav Arora, Pepa Atanasova, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>Stance Detection and Fact Checking</title>
      <link>https://copenlu.github.io/project/fact-checking/</link>
      <pubDate>Sun, 30 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/project/fact-checking/</guid>
      <description>We are interested in studying method to determine the attitude expressed in a text towards a topic (stance detection), such as determining if a tweet expresses a positive, negative or neutral stance towards a political entity. One additional challenge we are exploring is stance detection in a conversational context, where the stance depends on the context of the conversation. Fact checking using textual data can be framed very similarly, namely as if an evidence document agrees with, disagrees with or is topically unrelated to a headline or claim.</description>
    </item>
    
    <item>
      <title>2 Papers Accepted to ACL 2020</title>
      <link>https://copenlu.github.io/news/2-papers-accepted-to-acl-2020/</link>
      <pubDate>Sun, 05 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/2-papers-accepted-to-acl-2020/</guid>
      <description>2 papers by CopeNLU authors are accepted to appear at ACL 2020. One paper is on explainable fact checking, providing the first study of how fact checking explanations can be generated automatically based on claim content, and how this task can be modelled jointly with veracity prediction; whereas the other one is on script conversion, proposing a novel Chinese character conversion model that can disambiguate between mappings and convert between Chinese scripts.</description>
    </item>
    
  </channel>
</rss>