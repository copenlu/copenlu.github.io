<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>explainability on CopeNLU</title>
    <link>https://copenlu.github.io/tags/explainability/</link>
    <description>Recent content in explainability on CopeNLU</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 01 Oct 2022 00:00:00 +0200</lastBuildDate>
    
	<atom:link href="https://copenlu.github.io/tags/explainability/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>2 Papers Accepted to Coling 2022</title>
      <link>https://copenlu.github.io/talk/2022_08_coling/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0200</pubDate>
      
      <guid>https://copenlu.github.io/talk/2022_08_coling/</guid>
      <description>2 papers by CopeNLU authors on probing question answering models are accepted to appear at Coling 2022.
Machine Reading, Fast and Slow: When Do Models &amp;lsquo;Understand&amp;rsquo; Language?. Sagnik Ray Choudhury, Anna Rogers, Isabelle Augenstein.
Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?. Sagnik Ray Choudhury, Nikita Bhutani, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>3 Papers Accepted to NAACL 2022</title>
      <link>https://copenlu.github.io/talk/2022_07_naacl/</link>
      <pubDate>Fri, 01 Jul 2022 00:00:00 +0200</pubDate>
      
      <guid>https://copenlu.github.io/talk/2022_07_naacl/</guid>
      <description>3 papers by CopeNLU authors are accepted to appear at NAACL 2022, which are on the topics of hatespeech detection, misinformation detection and multilingual probing.
Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection. Indira Sen, Mattia Samory, Claudia Wagner, Isabelle Augenstein.
A Survey on Stance Detection for Mis- and Disinformation Identification. Momchil Hardalov, Arnav Arora, Preslav Nakov, Isabelle Augenstein.
Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models.</description>
    </item>
    
    <item>
      <title>2 Papers Accepted to AAAI 2022</title>
      <link>https://copenlu.github.io/talk/2021_12_aaai/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0100</pubDate>
      
      <guid>https://copenlu.github.io/talk/2021_12_aaai/</guid>
      <description>2 papers by CopeNLU authors are accepted to appear at AAAI 2022. One paper is on explanation generation, demonstrating how directly optimising for diagnostic properties of explanations, such as faithfulness, data consistency and confidence indication, can improve explanation quality. The other paper presents the most comprehensive study of cross-lingual stance detection to date, and proposes methods for learning with limited labelled data across languages and domains.
Diagnostics-Guided Explanation Generation. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>Paper Accepted to IJCAI 2021</title>
      <link>https://copenlu.github.io/talk/2021_04_ijcai/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0200</pubDate>
      
      <guid>https://copenlu.github.io/talk/2021_04_ijcai/</guid>
      <description>A paper by CopeNLU author is accepted to appear at IJCAI 2021. The paper studies how to perform complex claim verification on naturally occurring political claims with multiple hops over evidence chunks.
Multi-Hop Fact Checking of Political Claims. Wojciech Ostrowski, Arnav Arora, Pepa Atanasova, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>2 Papers Accepted to ACL 2021</title>
      <link>https://copenlu.github.io/talk/2021_04_acl/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0200</pubDate>
      
      <guid>https://copenlu.github.io/talk/2021_04_acl/</guid>
      <description>2 papers by CopeNLU authors are accepted to appear at ACL 2021. One paper is on interpretability, examining how sparsity affects our ability to use attention as an explainability tool; whereas the other one is on scientific document understanding, introducing a new dataset for the task of cite-worthiness detection in scientific articles.
Is Sparse Attention more Interpretable? Clara Meister, Stefan Lazov, Isabelle Augenstein, Ryan Cotterell.
CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding.</description>
    </item>
    
  </channel>
</rss>