<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>News on CopeNLU</title>
    <link>https://copenlu.github.io/talk/</link>
    <description>Recent content in News on CopeNLU</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://copenlu.github.io/talk/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Oustanding paper award at EACL 2024</title>
      <link>https://copenlu.github.io/talk/2024_03_eacl/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2024_03_eacl/</guid>
      <description>We are honoured to share that our paper on measuring the fragility of natural language inference models has won an outstanding paper award at EACL 2024. The paper is based on the MSc thesis of Zhaoqi Liu, who was supervised by Isabelle Augenstein and Erik Arakelyan.
Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models. Erik Arakelyan, Zhaoqi Liu, Isabelle Augenstein.
   </description>
    </item>
    
    <item>
      <title>PhD and postdoc positions available at Pioneer Centre for AI</title>
      <link>https://copenlu.github.io/talk/2024_03_positions/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2024_03_positions/</guid>
      <description>A PhD and two postdoc positions on natural language understanding are available. The positions are funded by the Pioneer Centre for AI. Read more about reasons to join us here. You can read more about the positions at the Pioneer Centre here.
PhD Fellowship on Factual Text Generation While recent large language models demonstrate surprising fluency and predictive capabilities in their generated text, they have been demonstrated to generate factual inaccuracies even when they have encoded truthful information.</description>
    </item>
    
    <item>
      <title>5 Papers Accepted to EMNLP 2023</title>
      <link>https://copenlu.github.io/talk/2023_12_emnlp/</link>
      <pubDate>Tue, 05 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2023_12_emnlp/</guid>
      <description>5 papers by CopeNLU authors are accepted to appear at EMNLP 2023, on topics ranging from explainability to language modelling.
Explaining Interactions Between Text Spans. Sagnik Ray Choudhury, Pepa Atanasova, Isabelle Augenstein.
Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions. Lucie-Aimée Kaffee, Arnav Arora, Isabelle Augenstein.
Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing. Lucie-Aimée Kaffee, Arnav Arora, Zeerak Talat, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>PhD position available in context of ERC Starting Grant project ExplainYourself</title>
      <link>https://copenlu.github.io/talk/2023_11_erc/</link>
      <pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2023_11_erc/</guid>
      <description>A PhD fellowship on explainable natural language understanding is available in CopeNLU. The successful candidate will be supervised by Isabelle Augenstein and Pepa Atanasova. The positions are offered in the context of an ERC Starting Grant on &amp;lsquo;Explainable and Robust Automatic Fact Checking (ExplainYourself)&amp;rsquo;. ERC Starting Grant is a highly competitive funding program by the European Research Council to support the most talented early-career scientists in Europe with funding for a period of 5 years for blue-skies research to build up or expand their research groups.</description>
    </item>
    
    <item>
      <title>ExplainYourself Project Kick-Off</title>
      <link>https://copenlu.github.io/talk/2023_09_explainyourself/</link>
      <pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2023_09_explainyourself/</guid>
      <description>On 1 September 2023, the ERC Starting Grant project ExplainYourself on &amp;lsquo;Explainable and Robust Automatic Fact Checking&amp;rsquo; is officially kicking off. ERC Starting Grant is a highly competitive fellowship programme by the European Research Council to support talented early-career scientists who show potential to be a research leader. It provides funding of blue-skies research for a period of up to 5 years.
ExplainYourself proposes to study explainable automatic fact checking, the task of automatically predicting the veracity of textual claims using machine learning (ML) methods, while also producing explanations about how the model arrived at the prediction.</description>
    </item>
    
    <item>
      <title>4 Papers Accepted to ACL 2023</title>
      <link>https://copenlu.github.io/talk/2023_05_acl/</link>
      <pubDate>Mon, 22 May 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2023_05_acl/</guid>
      <description>4 papers by CopeNLU authors are accepted to appear at ACL 2023. The papers make contributions within faithfulness of explanations, measuring intersectional biases, event extraction and few-shot stance detection.
Topic-Guided Sampling For Data-Efficient Multi-Domain Stance Detection. Erik Arakelyan, Arnav Arora, Isabelle Augenstein.
Faithfulness Tests for Natural Language Explanations. Pepa Atanasova, Oana-Maria Camburu, Christina Lioma, Thomas Lukasiewicz, Jakob Grue Simonsen, Isabelle Augenstein.
Measuring Intersectional Biases in Historical Documents. Nadav Borenstein, Karolina Stańczak, Thea Rolskov, Natacha Klein Käfer, Natália da Silva Perez, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>Positions available in context of ERC Starting Grant project ExplainYourself</title>
      <link>https://copenlu.github.io/talk/2022_11_erc/</link>
      <pubDate>Tue, 22 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2022_11_erc/</guid>
      <description>Three PhD fellowships and two postdoc positions on explainable stance detection are available in CopeNLU. The positions are offered in the context of an ERC Starting Grant on &amp;lsquo;Explainable and Robust Automatic Fact Checking (ExplainYourself)&amp;rsquo;. ERC Starting Grant is a highly competitive funding program by the European Research Council to support the most talented early-career scientists in Europe with funding for a period of 5 years for blue-skies research to build up or expand their research groups.</description>
    </item>
    
    <item>
      <title>Isabelle Augenstein becomes Denmark&#39;s youngest female full professor</title>
      <link>https://copenlu.github.io/talk/2022_10_promotion/</link>
      <pubDate>Mon, 17 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2022_10_promotion/</guid>
      <description>Isabelle Augenstein has been promoted to full professor, making her the youngest ever female full professor in Denmark. The former officially reported youngest female full professor was appointed in 2008 when she was 34 years old. Read more University of Copenhagen&amp;rsquo;s press release.</description>
    </item>
    
    <item>
      <title>2 Papers Accepted to EMNLP 2022</title>
      <link>https://copenlu.github.io/talk/2022_11_emnlp/</link>
      <pubDate>Mon, 10 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2022_11_emnlp/</guid>
      <description>2 papers by CopeNLU authors are accepted to appear at EMNLP 2022, which are on scholarly document understanding.
Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection. Indira Sen, Mattia Samory, Claudia Wagner, Isabelle Augenstein.
Neighborhood Contrastive Learning for Scientific Document Representations with Citation Embeddings. Malte Ostendorff, Nils Rethmeier, Isabelle Augenstein, Bela Gipp, Georg Rehm.</description>
    </item>
    
    <item>
      <title>2 Papers Accepted to Coling 2022</title>
      <link>https://copenlu.github.io/talk/2022_08_coling/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2022_08_coling/</guid>
      <description>2 papers by CopeNLU authors on probing question answering models are accepted to appear at Coling 2022.
Machine Reading, Fast and Slow: When Do Models &amp;lsquo;Understand&amp;rsquo; Language?. Sagnik Ray Choudhury, Anna Rogers, Isabelle Augenstein.
Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?. Sagnik Ray Choudhury, Nikita Bhutani, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>3 Papers Accepted to NAACL 2022</title>
      <link>https://copenlu.github.io/talk/2022_07_naacl/</link>
      <pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2022_07_naacl/</guid>
      <description>3 papers by CopeNLU authors are accepted to appear at NAACL 2022, which are on the topics of hatespeech detection, misinformation detection and multilingual probing.
Counterfactually Augmented Data and Unintended Bias: The Case of Sexism and Hate Speech Detection. Indira Sen, Mattia Samory, Claudia Wagner, Isabelle Augenstein.
A Survey on Stance Detection for Mis- and Disinformation Identification. Momchil Hardalov, Arnav Arora, Preslav Nakov, Isabelle Augenstein.
Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models.</description>
    </item>
    
    <item>
      <title>2 Papers Accepted to AAAI 2022</title>
      <link>https://copenlu.github.io/talk/2021_12_aaai/</link>
      <pubDate>Wed, 01 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2021_12_aaai/</guid>
      <description>2 papers by CopeNLU authors are accepted to appear at AAAI 2022. One paper is on explanation generation, demonstrating how directly optimising for diagnostic properties of explanations, such as faithfulness, data consistency and confidence indication, can improve explanation quality. The other paper presents the most comprehensive study of cross-lingual stance detection to date, and proposes methods for learning with limited labelled data across languages and domains.
Diagnostics-Guided Explanation Generation. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>EXPANSE Project Kick-Off</title>
      <link>https://copenlu.github.io/talk/2021_09_people/</link>
      <pubDate>Tue, 31 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2021_09_people/</guid>
      <description>On 1 September 2021, the DFF Sapere Aude project EXPANSE on &amp;lsquo;Learning to Explain Attitudes on Social Media&amp;rsquo; is officially kicking off. Sapere Aude is a program by the Independent Research Fund Denmark (DFF) to support the most talented younger researchers in Denmark with funding for blue-skies research to build up or expand their research groups.
The EXPANSE project will study attitudes voiced on social media, examining what is said, and explaining it by examining why, how and by whom attitudes are stated.</description>
    </item>
    
    <item>
      <title>3 Papers Accepted to EMNLP 2021</title>
      <link>https://copenlu.github.io/talk/2021_08_emnlp/</link>
      <pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2021_08_emnlp/</guid>
      <description>3 papers by CopeNLU authors are accepted to appear at EMNLP 2021. The topics of these include stance detection, exaggeration detection, and counterfactually augmented data.
Cross-Domain Label-Adaptive Stance Detection. Momchil Hardalov, Arnav Arora, Preslav Nakov, Isabelle Augenstein. In Proceedings of EMNLP.
How Does Counterfactually Augmented Data Impact Models for Social Computing Constructs?. Indira Sen, Mattia Samory, Fabian Flöck, Claudia Wagner, Isabelle Augenstein. In Proceedings of EMNLP.
Semi-Supervised Exaggeration Detection of Health Science Press Releases.</description>
    </item>
    
    <item>
      <title>Paper Accepted to IJCAI 2021</title>
      <link>https://copenlu.github.io/talk/2021_04_ijcai/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2021_04_ijcai/</guid>
      <description>A paper by CopeNLU author is accepted to appear at IJCAI 2021. The paper studies how to perform complex claim verification on naturally occurring political claims with multiple hops over evidence chunks.
Multi-Hop Fact Checking of Political Claims. Wojciech Ostrowski, Arnav Arora, Pepa Atanasova, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>2 Papers Accepted to ACL 2021</title>
      <link>https://copenlu.github.io/talk/2021_04_acl/</link>
      <pubDate>Mon, 05 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2021_04_acl/</guid>
      <description>2 papers by CopeNLU authors are accepted to appear at ACL 2021. One paper is on interpretability, examining how sparsity affects our ability to use attention as an explainability tool; whereas the other one is on scientific document understanding, introducing a new dataset for the task of cite-worthiness detection in scientific articles.
Is Sparse Attention more Interpretable? Clara Meister, Stefan Lazov, Isabelle Augenstein, Ryan Cotterell.
CiteWorth: Cite-Worthiness Detection for Improved Scientific Document Understanding.</description>
    </item>
    
    <item>
      <title>Paper Accepted to EACL 2021</title>
      <link>https://copenlu.github.io/talk/2021_01_eacl/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2021_01_eacl/</guid>
      <description>A paper by CopeNLU author is accepted to appear at EACL 2021. The paper aims to bridge the gap between high- and low-resource languages by investigating to what degree cross-lingual models share structural information about languages.
Does Typological Blinding Impede Cross-Lingual Sharing?. Johannes Bjerva, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>Positions available in context of Sapere Aude Research Leader Fellowship on Explainable Stance Detection</title>
      <link>https://copenlu.github.io/talk/2020_10_phd/</link>
      <pubDate>Thu, 19 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2020_10_phd/</guid>
      <description>Two PhD fellowships and two postdoc positions on explainable stance detection are available in CopeNLU. The PhD fellowships and one of the postdoc positions are offered in the context of a DFF Sapere Aude research leader fellowship on `Learning to Explain Attitudes on Social Media (EXPANSE)´. Sapere Aude is a program by the Independent Research Fund Denmark (DFF) to support the most talented younger researchers in Denmark with funding for blue-skies research to build up or expand their research groups.</description>
    </item>
    
    <item>
      <title>7 Papers Accepted to EMNLP 2020</title>
      <link>https://copenlu.github.io/talk/2020_09_emnlp/</link>
      <pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2020_09_emnlp/</guid>
      <description>7 CopeNLU papers are accepted to appear at EMNLP 2020. The topics of these include fact checking, explainability, domain adaptation, transfer learning, QA and improving peer review.
A Diagnostic Study of Explainability Techniques for Text Classification. Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, Isabelle Augenstein. In Proceedings of EMNLP.
Generating Label Cohesive and Well-Formed Adversarial Claims. Pepa Atanasova&amp;#42;, Dustin Wright&amp;#42;, Isabelle Augenstein. In Proceedings of EMNLP.
Transformer Based Multi-Source Domain Adaptation.</description>
    </item>
    
    <item>
      <title>2 Papers Accepted to ACL 2020</title>
      <link>https://copenlu.github.io/talk/2020_04_acl/</link>
      <pubDate>Sun, 05 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2020_04_acl/</guid>
      <description>2 papers by CopeNLU authors are accepted to appear at ACL 2020. One paper is on explainable fact checking, providing the first study of how fact checking explanations can be generated automatically based on claim content, and how this task can be modelled jointly with veracity prediction; whereas the other one is on script conversion, proposing a novel Chinese character conversion model that can disambiguate between mappings and convert between Chinese scripts.</description>
    </item>
    
    <item>
      <title>4 papers to be presented at EMNLP 2019</title>
      <link>https://copenlu.github.io/talk/2019_11_emnlp/</link>
      <pubDate>Fri, 01 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2019_11_emnlp/</guid>
      <description>4 papers by CopeNLU authors are to be presented at EMNLP 2019 and co-located events, on fact checking and disinformation, as well as on multi-task and multi-lingual learning.
MultiFC: A Real-World Multi-Domain Dataset for Evidence-Based Fact Checking of Claims. Isabelle Augenstein, Christina Lioma, Dongsheng Wang, Lucas Chaves Lima, Casper Hansen, Christian Hansen, Jakob Grue Simonsen. In Proceedings of EMNLP-IJCNLP 2019.
Mapping (Dis-)Information Flow about the MH17 Plane Crash. Mareike Hartmann, Yevgeniy Golovchenko, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>2 Papers Accepted to ACL 2019</title>
      <link>https://copenlu.github.io/talk/2019_05_acl/</link>
      <pubDate>Tue, 14 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2019_05_acl/</guid>
      <description>2 papers by CopeNLU authors are accepted to appear at ACL 2019. One paper is on uncovering probabilistic implications in typological knowledge bases, following up from our NAACL 2019 paper on generative linguistic typology; whereas the other one is on unsupervised discovery of gendered language, utilising the multi-view autoencoder introduced in our NAACL 219 paper.
Uncovering Probabilistic Implications in Typological Knowledge Bases. Johannes Bjerva, Yova Kementchedjhieva, Ryan Cotterell, Isabelle Augenstein.</description>
    </item>
    
    <item>
      <title>3 Papers Accepted to NAACL 2019</title>
      <link>https://copenlu.github.io/talk/2019_02_naacl/</link>
      <pubDate>Fri, 22 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2019_02_naacl/</guid>
      <description>3 papers by CopeNLU authors are accepted to appear at NAACL 2019. Topics span from population of typological knowledge bases and weak supervision from disparate lexica to frame detection in online fora.
A Probabilistic Generative Model of Linguistic Typology. Johannes Bjerva, Yova Kementchedjhieva, Ryan Cotterell, Isabelle Augenstein.
Combining Disparate Sentiment Lexica with a Multi-View Variational Autoencoder. Alexander Hoyle, Lawrence Wolf-Sonkin, Hanna Wallach, Ryan Cotterell, Isabelle Augenstein.
Issue Framing in Online Discussion Fora.</description>
    </item>
    
  </channel>
</rss>