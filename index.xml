<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CopeNLU on CopeNLU</title>
    <link>https://copenlu.github.io/</link>
    <description>Recent content in CopeNLU on CopeNLU</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models</title>
      <link>https://copenlu.github.io/publication/2024_eacl_arakelyan/</link>
      <pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_eacl_arakelyan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>5 Papers Accepted to EMNLP 2023</title>
      <link>https://copenlu.github.io/talk/2023_12_emnlp/</link>
      <pubDate>Tue, 05 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2023_12_emnlp/</guid>
      <description>&lt;p&gt;5 papers by CopeNLU authors are accepted to appear at EMNLP 2023, on topics ranging from explainability to language modelling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2023_emnlp_choudhury/&#34;&gt;Explaining Interactions Between Text Spans&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/sagnik-ray-choudhury/&#34;&gt;Sagnik Ray Choudhury&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/pepa-atanasova/&#34;&gt;Pepa Atanasova&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2023_emnlp_kaffee-wikipedia/&#34;&gt;Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/lucie-aim%C3%A9e-kaffee/&#34;&gt;Lucie-Aimée Kaffee&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/arnav-arora&#34;&gt;Arnav Arora&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2023_emnlp_kaffee-dual/&#34;&gt;Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/lucie-aim%C3%A9e-kaffee/&#34;&gt;Lucie-Aimée Kaffee&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/arnav-arora&#34;&gt;Arnav Arora&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/zeerak-talat&#34;&gt;Zeerak Talat&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2023_emnlp_sen/&#34;&gt;People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/indira-sen/&#34;&gt;Indira Sen&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/dennis-assenmacher/&#34;&gt;Dennis Assenmacher&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/mattia-samory&#34;&gt;Mattia Samory&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/wil-van-der-aalst/&#34;&gt;Wil van der Aalst&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/claudia-wagner/&#34;&gt;Claudia Wagner&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2023_emnlp_borenstein/&#34;&gt;PHD: Pixel-Based Language Modeling of Historical Documents&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/nadav-borenstein/&#34;&gt;Nadav Borenstein&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/phillip-rust/&#34;&gt;Philipp Rust&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/desmond-elliott/&#34;&gt;Desmond Elliott&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Grammatical Gender&#39;s Influence on Distributional Semantics: A Causal Perspective</title>
      <link>https://copenlu.github.io/publication/2023_arxiv_stanczak/</link>
      <pubDate>Thu, 30 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_arxiv_stanczak/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Invisible Women in Digital Diplomacy: A Multidimensional Framework for Online Gender Bias Against Women Ambassadors Worldwide</title>
      <link>https://copenlu.github.io/publication/2023_arxiv_golovchenko/</link>
      <pubDate>Wed, 29 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_arxiv_golovchenko/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output</title>
      <link>https://copenlu.github.io/publication/2023_arxiv_wang/</link>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_arxiv_wang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Social Bias Probing: Fairness Benchmarking for Language Models</title>
      <link>https://copenlu.github.io/publication/2023_arxiv_manerba/</link>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_arxiv_manerba/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quantifying Gender Bias Towards Politicians in Cross-Lingual Language Models</title>
      <link>https://copenlu.github.io/publication/2023_plosone_stanczak/</link>
      <pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_plosone_stanczak/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PhD position available in context of ERC Starting Grant project ExplainYourself</title>
      <link>https://copenlu.github.io/talk/2023_11_erc/</link>
      <pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2023_11_erc/</guid>
      <description>&lt;p&gt;A PhD fellowship on explainable natural language understanding is available in CopeNLU. The successful candidate will be supervised by &lt;a href=&#34;http://isabelleaugenstein.github.io/&#34;&gt;Isabelle Augenstein&lt;/a&gt; and &lt;a href=&#34;https://apepa.github.io/&#34;&gt;Pepa Atanasova&lt;/a&gt;. The positions are offered in the context of an &lt;a href=&#34;https://erc.europa.eu/apply-grant/starting-grant&#34;&gt;ERC Starting Grant&lt;/a&gt; on &amp;lsquo;&lt;a href=&#34;https://erc.europa.eu/news/erc-2021-starting-grants-results&#34;&gt;Explainable and Robust Automatic Fact Checking (ExplainYourself)&lt;/a&gt;&amp;rsquo;. ERC Starting Grant is a highly competitive funding program by the &lt;a href=&#34;https://erc.europa.eu/homepage&#34;&gt;European Research Council&lt;/a&gt; to support the most talented early-career scientists in Europe with funding for a period of 5 years for blue-skies research to build up or expand their research groups.&lt;/p&gt;

&lt;p&gt;ExplainYourself proposes to study explainable automatic fact checking, the task of automatically predicting the veracity of textual claims using machine learning (ML) methods, while also producing explanations about how the model arrived at the prediction. Automatic fact checking methods often use opaque deep neural network models, whose inner workings cannot easily be explained. Especially for complex tasks such as automatic fact checking, this hinders greater adoption, as it is unclear to users when the models&amp;rsquo; predictions can be trusted. Existing explainable ML methods partly overcome this by reducing the task of explanation generation to highlighting the right rationale. While a good first step, this does not fully explain how a ML model arrived at a prediction. For knowledge intensive natural language understanding (NLU) tasks such as fact checking, a ML model needs to learn complex relationships between the claim, multiple evidence documents, and common sense knowledge in addition to retrieving the right evidence. There is currently no explainability method that aims to illuminate this highly complex process. In addition, existing approaches are unable to produce diverse explanations, geared towards users with different information needs. ExplainYourself radically departs from existing work in proposing methods for explainable fact checking that more accurately reflect how fact checking models make decisions, and are useful to diverse groups of end users. It is expected that these innovations will apply to explanation generation for other knowledge-intensive NLU tasks, such as question answering or entity linking.&lt;/p&gt;

&lt;p&gt;In addition to the principle investigator, PhD students and postdocs, the project team will also include collaborators from CopeNLU as well as external collaborators. Two PhD students as well as a postdoc have already been recruited as a result of earlier calls, and the project officially &lt;a href=&#34;https://copenlu.github.io/talk/2023_09_explainyourself/&#34;&gt;kicked off in September 2023&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read more about reasons to join us &lt;a href=&#34;https://copenlu.github.io/post/why-ucph/&#34;&gt;here&lt;/a&gt;. You can read more about the position and apply &lt;a href=&#34;https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&amp;departmentId=18970&amp;ProjectId=160498&amp;MediaId=5&amp;SkipAdvertisement=false&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection</title>
      <link>https://copenlu.github.io/publication/2023_emnlp_sen/</link>
      <pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_emnlp_sen/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PHD: Pixel-Based Language Modeling of Historical Documents</title>
      <link>https://copenlu.github.io/publication/2023_emnlp_borenstein/</link>
      <pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_emnlp_borenstein/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Explaining Interactions Between Text Spans</title>
      <link>https://copenlu.github.io/publication/2023_emnlp_choudhury/</link>
      <pubDate>Tue, 24 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_emnlp_choudhury/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing</title>
      <link>https://copenlu.github.io/publication/2023_emnlp_kaffee-dual/</link>
      <pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_emnlp_kaffee-dual/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions</title>
      <link>https://copenlu.github.io/publication/2023_emnlp_kaffee-wikipedia/</link>
      <pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_emnlp_kaffee-wikipedia/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Adapting Neural Link Predictors for Complex Query Answering</title>
      <link>https://copenlu.github.io/publication/2023_neurips_arakelyan/</link>
      <pubDate>Thu, 21 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_neurips_arakelyan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ExplainYourself Project Kick-Off</title>
      <link>https://copenlu.github.io/talk/2023_09_explainyourself/</link>
      <pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2023_09_explainyourself/</guid>
      <description>&lt;p&gt;On 1 September 2023, the ERC Starting Grant project ExplainYourself on &amp;lsquo;Explainable and Robust Automatic Fact Checking&amp;rsquo; is officially kicking off. &lt;a href=&#34;https://erc.europa.eu/apply-grant/starting-grant&#34;&gt;ERC Starting Grant&lt;/a&gt; is a highly competitive fellowship programme by the European Research Council to support talented early-career scientists who show potential to be a research leader. It provides funding of blue-skies research for a period of up to 5 years.&lt;/p&gt;

&lt;p&gt;ExplainYourself proposes to study explainable automatic fact checking, the task of automatically predicting the veracity of textual claims using machine learning (ML) methods, while also producing explanations about how the model arrived at the prediction. Automatic fact checking methods often use opaque deep neural network models, whose inner workings cannot easily be explained. Especially for complex tasks such as automatic fact checking, this hinders greater adoption, as it is unclear to users when the models’ predictions can be trusted. Existing explainable ML methods partly overcome this by reducing the task of explanation generation to highlighting the right rationale. While a good first step, this does not fully explain how a ML model arrived at a prediction. For knowledge intensive natural language understanding (NLU) tasks such as fact checking, a ML model needs to learn complex relationships between the claim, multiple evidence documents, and common sense knowledge in addition to retrieving the right evidence. There is currently no explainability method that aims to illuminate this highly complex process. In addition, existing approaches are unable to produce diverse explanations, geared towards users with different information needs. ExplainYourself radically departs from existing work in proposing methods for explainable fact checking that more accurately reflect how fact checking models make decisions, and are useful to diverse groups of end users. It is expected that these innovations will apply to explanation generation for other knowledge-intensive NLU tasks, such as question answering or entity linking.&lt;/p&gt;

&lt;p&gt;The following &lt;a href=&#34;www.copenlu.com/#people&#34;&gt;researchers&lt;/a&gt; affiliated with the ExplainYourself project are joining CopeNLU on 1 September 2023:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://haeunyu.github.io/&#34;&gt;Haeun Yu&lt;/a&gt; (PhD Student), whose main research interests include enhancing explainability in fact-checking and transparency of knowledge-enhanced LM;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://scholar.google.com.hk/citations?hl=zh-CN&amp;user=KD_0W9sAAAAJ&#34;&gt;Jingyi Sun&lt;/a&gt; (PhD student), whose research interests include explainability, fact-checking, and question answering.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;They will both be supervised by &lt;a href=&#34;http://isabelleaugenstein.github.io/&#34;&gt;Isabelle Augenstein&lt;/a&gt; and &lt;a href=&#34;https://apepa.github.io/&#34;&gt;Pepa Atanasova&lt;/a&gt;.
A postdoctoral researcher with a focus on human-centered explainability methods for fact checking is expected to join the team in Spring 2024, and there will soon be openings for further positions for a start in autumn 2024.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
