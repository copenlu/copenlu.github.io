<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CopeNLU on CopeNLU</title>
    <link>https://copenlu.github.io/</link>
    <description>Recent content in CopeNLU on CopeNLU</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>A Meta-Evaluation of Style and Attribute Transfer Metrics</title>
      <link>https://copenlu.github.io/publication/2025_emnlp_pauli/</link>
      <pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_emnlp_pauli/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Explainability and Interpretability of Multilingual Large Language Models: A Survey</title>
      <link>https://copenlu.github.io/publication/2025_emnlp_resck/</link>
      <pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_emnlp_resck/</guid>
      <description></description>
    </item>
    
    <item>
      <title>FLARE: Faithful Logic-Aided Reasoning and Exploration</title>
      <link>https://copenlu.github.io/publication/2025_emnlp_arakelyan/</link>
      <pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_emnlp_arakelyan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Graph-Guided Textual Explanation Generation Framework</title>
      <link>https://copenlu.github.io/publication/2025_emnlp_yuan/</link>
      <pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_emnlp_yuan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Multi-Modal Framing Analysis of News</title>
      <link>https://copenlu.github.io/publication/2025_emnlp_arora/</link>
      <pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_emnlp_arora/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Presumed Cultural Identity: How Names Shape LLM Responses</title>
      <link>https://copenlu.github.io/publication/2025_emnlp_pawar/</link>
      <pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_emnlp_pawar/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Self-Critique and Refinement for Faithful Natural Language Explanations</title>
      <link>https://copenlu.github.io/publication/2025_emnlp_wang/</link>
      <pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_emnlp_wang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Unstructured Evidence Attribution for Long Context Query Focused Summarization</title>
      <link>https://copenlu.github.io/publication/2025_emnlp_wright/</link>
      <pubDate>Thu, 21 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_emnlp_wright/</guid>
      <description></description>
    </item>
    
    <item>
      <title>BiasGym: Fantastic Biases and How to Find (and Remove) Them</title>
      <link>https://copenlu.github.io/publication/2025_arxiv_islam/</link>
      <pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_arxiv_islam/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models</title>
      <link>https://copenlu.github.io/publication/2025_arxiv_yu/</link>
      <pubDate>Wed, 13 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_arxiv_yu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Reliable Evaluation Protocol for Low-Precision Retrieval</title>
      <link>https://copenlu.github.io/publication/2025_arxiv_yang/</link>
      <pubDate>Thu, 07 Aug 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_arxiv_yang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PhD fellowships for start in Spring or Autumn 2026</title>
      <link>https://copenlu.github.io/news/phd-fellowships-for-start-in-spring-or-autumn-2026/</link>
      <pubDate>Thu, 26 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/news/phd-fellowships-for-start-in-spring-or-autumn-2026/</guid>
      <description>

&lt;p&gt;Would you like to join our lab as a PhD student in 2026? We have several openings. Read more about reasons to join CopeNLU &lt;a href=&#34;https://copenlu.github.io/post/why-ucph/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;start-in-spring-2026&#34;&gt;Start in Spring 2026&lt;/h2&gt;

&lt;p&gt;A fully funded 3-year PhD fellowship on &lt;strong&gt;explainable natural language understanding&lt;/strong&gt; for a start in &lt;strong&gt;Spring 2026&lt;/strong&gt; is available as part of the &amp;lsquo;&lt;a href=&#34;https://erc.europa.eu/news/erc-2021-starting-grants-results&#34;&gt;ExplainYourself project&lt;/a&gt; on Explainable and Robust Automatic Fact Checking. The position requires a Master&amp;rsquo;s degree. The successful candidate will be supervised by &lt;a href=&#34;http://isabelleaugenstein.github.io/&#34;&gt;Isabelle Augenstein&lt;/a&gt; and co-supervised by &lt;a href=&#34;https://apepa.github.io/&#34;&gt;Pepa Atanasova&lt;/a&gt;.
Read more about the position and apply &lt;a href=&#34;https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&amp;departmentId=18970&amp;ProjectId=164789&amp;MediaId=5&amp;SkipAdvertisement=false&#34;&gt;here&lt;/a&gt; by &lt;strong&gt;31 October 2025&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The project is funded by an ERC Starting Grant, a highly competitive funding program by the &lt;a href=&#34;https://erc.europa.eu/homepage&#34;&gt;European Research Council&lt;/a&gt; which supports the most talented early-career scientists in Europe with funding for a period of 5 years for blue-skies research to build up or expand their research groups.&lt;/p&gt;

&lt;p&gt;ExplainYourself proposes to study explainable automatic fact checking, the task of automatically predicting the veracity of textual claims using machine learning (ML) methods, while also producing explanations about how the model arrived at the prediction. Automatic fact checking methods often use opaque deep neural network models, whose inner workings cannot easily be explained. Especially for complex tasks such as automatic fact checking, this hinders greater adoption, as it is unclear to users when the models&amp;rsquo; predictions can be trusted. Existing explainable ML methods partly overcome this by reducing the task of explanation generation to highlighting the right rationale. While a good first step, this does not fully explain how a ML model arrived at a prediction. For knowledge intensive natural language understanding (NLU) tasks such as fact checking, a ML model needs to learn complex relationships between the claim, multiple evidence documents, and common sense knowledge in addition to retrieving the right evidence. There is currently no explainability method that aims to illuminate this highly complex process. In addition, existing approaches are unable to produce diverse explanations, geared towards users with different information needs. ExplainYourself radically departs from existing work in proposing methods for explainable fact checking that more accurately reflect how fact checking models make decisions, and are useful to diverse groups of end users. It is expected that these innovations will apply to explanation generation for other knowledge-intensive NLU tasks, such as question answering or entity linking.&lt;/p&gt;

&lt;p&gt;In addition to the principle investigator, PhD students and postdocs, the project team includes collaborators from CopeNLU as well as external collaborators. Two PhD students as well as a postdoc have already been recruited as a result of earlier calls, and the project officially &lt;a href=&#34;https://copenlu.github.io/talk/2023_09_explainyourself/&#34;&gt;kicked off in September 2023&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;start-in-autumn-2026&#34;&gt;Start in Autumn 2026&lt;/h2&gt;

&lt;p&gt;For a start in &lt;strong&gt;Autumn 2026&lt;/strong&gt;, we are considering candidates on any topic aligned with the &lt;a href=&#34;https://www.copenlu.com/#projects&#34;&gt;focus areas of our lab&lt;/a&gt;. Candidates should express their interest by applying to the &lt;a href=&#34;https://ellis.eu/news/ellis-phd-program-call-for-applications-2025&#34;&gt;ELLIS PhD programme&lt;/a&gt; by &lt;strong&gt;31 October 2025&lt;/strong&gt;, naming Isabelle Augenstein as a supervisor. ELLIS is a pan-European recruitment vehicle for PhD students and does not provide funded PhD fellowships, though it offers networking opportunities, and opportunities to obtain travel funding.&lt;/p&gt;

&lt;p&gt;Successful candidates will be supported in applying for funded fellowship opportunities, including those offered by the &lt;a href=&#34;https://ddsa.dk/about-ddsa-fellowship-programme/&#34;&gt;Danish Data Science Academy (DDSA)&lt;/a&gt; and the &lt;a href=&#34;https://www.daracademy.dk/fellowships&#34;&gt;Danish Advanced Research Academy (DARA)&lt;/a&gt;. Additional fully funded PhD positions may become available through the &lt;a href=&#34;https://www.aicentre.dk/jobs&#34;&gt;Pioneer Centre for AI&lt;/a&gt;. Candidates without Master&amp;rsquo;s degrees may be eligible.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modeling Public Perceptions of Science in Media</title>
      <link>https://copenlu.github.io/publication/2025_arxiv_pei/</link>
      <pubDate>Fri, 20 Jun 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_arxiv_pei/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Pre-ACL 2025 Workshop in Copenhagen</title>
      <link>https://copenlu.github.io/post/pre-acl-ws/</link>
      <pubDate>Wed, 28 May 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/post/pre-acl-ws/</guid>
      <description>&lt;p&gt;We’re excited to welcome researchers and practitioners in Natural Language Processing, Generative AI, and Language Technology to a one-day workshop on 26 July 2025 – just ahead of ACL 2025 in Vienna.&lt;/p&gt;

&lt;p&gt;This is a unique opportunity to connect the Danish NLP community with leading international experts through a dynamic programme of talks, poster sessions, and round-table discussions designed to inspire collaboration and spark new ideas. 💡🤝&lt;/p&gt;

&lt;p&gt;Invited speakers include: 🎙️&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Professor Thamar Solorio (MBUZAI)&lt;/li&gt;
&lt;li&gt;Professor Mausam (IIT Delhi)&lt;/li&gt;
&lt;li&gt;Associate Professor Smaranda Muresan (Columbia University)&lt;/li&gt;
&lt;li&gt;Associate Professor David Jurgens (University of Michigan)&lt;/li&gt;
&lt;li&gt;Associate Professor Tanu Mitra (UW)&lt;/li&gt;
&lt;li&gt;Assistant Professor Nanyun Peng (UCLA)&lt;/li&gt;
&lt;li&gt;Assistant Professor Danish Pruthi (IISc Bangalore)&lt;/li&gt;
&lt;li&gt;Associate Professor Kai-Wei Chang (UCLA &amp;amp; Amazon Scholar)&lt;/li&gt;
&lt;li&gt;Assistant Professor Anjalie Field (Johns Hopkins University)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;📝 Registration is free, but spots are limited!&lt;/p&gt;

&lt;p&gt;📅 Sign-up deadline: 5 July 2025&lt;/p&gt;

&lt;p&gt;🔗 Register here: &lt;a href=&#34;https://www.aicentre.dk/events/pre-acl-2025-workshop&#34; target=&#34;_blank&#34;&gt;https://www.aicentre.dk/events/pre-acl-2025-workshop&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;🎯 Interested in presenting a poster? Submit your proposal by 16 June 2025.&lt;/p&gt;

&lt;p&gt;The Pre-ACL 2025 Workshop is supported by the DDSA - Danish Data Science Academy and Pioneer Centre for AI and is organised by Isabelle Augenstein, Pepa Atanasova, Arnav Arora, Dustin Wright, and Yoonna Jang.&lt;/p&gt;



&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://copenlu.github.io/post/pre-acl-ws/gallery/Pre-ACL-Workshop-2025.png&#34; &gt;
  &lt;img src=&#34;https://copenlu.github.io/post/pre-acl-ws/gallery/Pre-ACL-Workshop-2025.png&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  

  
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Community Moderation and the New Epistemology of Fact Checking on Social Media</title>
      <link>https://copenlu.github.io/publication/2025_arxiv_augenstein/</link>
      <pubDate>Tue, 27 May 2025 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2025_arxiv_augenstein/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
