<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CopeNLU on CopeNLU</title>
    <link>https://copenlu.github.io/</link>
    <description>Recent content in CopeNLU on CopeNLU</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Oustanding paper award at EACL 2024</title>
      <link>https://copenlu.github.io/talk/2024_03_eacl/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2024_03_eacl/</guid>
      <description>&lt;p&gt;We are honoured to share that our paper on measuring the fragility of natural language inference models has won an outstanding paper award at EACL 2024. The paper is based on the MSc thesis of Zhaoqi Liu, who was supervised by Isabelle Augenstein and Erik Arakelyan.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2024_eacl_arakelyan/&#34;&gt;Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/erik-arakelyan/&#34;&gt;Erik Arakelyan&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/zhaoqi liu/&#34;&gt;Zhaoqi Liu&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;



&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://copenlu.github.io/talk/2024_03_eacl/gallery/award.jpg&#34; &gt;
  &lt;img src=&#34;https://copenlu.github.io/talk/2024_03_eacl/gallery/award.jpg&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://copenlu.github.io/talk/2024_03_eacl/gallery/award2.jpg&#34; &gt;
  &lt;img src=&#34;https://copenlu.github.io/talk/2024_03_eacl/gallery/award2.jpg&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  

  
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>PhD and postdoc positions available at Pioneer Centre for AI</title>
      <link>https://copenlu.github.io/talk/2024_03_positions/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2024_03_positions/</guid>
      <description>

&lt;p&gt;A PhD and two postdoc positions on natural language understanding are available. The positions are funded by the &lt;a href=&#34;https://www.aicentre.dk/&#34;&gt;Pioneer Centre for AI&lt;/a&gt;. Read more about reasons to join us &lt;a href=&#34;https://copenlu.github.io/post/why-ucph/&#34;&gt;here&lt;/a&gt;. You can read more about the positions at the Pioneer Centre &lt;a href=&#34;https://www.aicentre.dk/jobs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;phd-fellowship-on-factual-text-generation&#34;&gt;PhD Fellowship on Factual Text Generation&lt;/h2&gt;

&lt;p&gt;While recent large language models demonstrate surprising fluency and predictive capabilities in their generated text, they have been demonstrated to generate factual inaccuracies even when they have encoded truthful information. This limits their utility and safety in real world scenarios where guarantees of factuality are needed. To address this, the project will explore methods for improving the factuality of text generation with respect to both objective real-world facts and provided source documents.&lt;/p&gt;

&lt;p&gt;We are looking for candidates with a background in computer science, machine learning, natural language processing, computational social science, or similar. The candidate should have an interest in automatic text generation and fact checking. They should also have an interest in interdisciplinary research endeavors, including at the Pioneer Center for AI. Early research experience, especially with empirical research methods, or relevant industry experience, will be a bonus.&lt;/p&gt;

&lt;p&gt;The principal supervisor is &lt;a href=&#34;mailto:augenstein@di.ku.dk&#34;&gt;Professor Isabelle Augenstein&lt;/a&gt; and the co-supervisor is &lt;a href=&#34;mailto:dw@di.ku.dk&#34;&gt;Dustin Wright&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Application deadline: 1 April 2024. &lt;a href=&#34;https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&amp;departmentId=18970&amp;ProjectId=161235&amp;MediaId=5&amp;SkipAdvertisement=false&#34;&gt;Apply here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;postdoctoral-fellowship-on-nlp-for-computational-social-science&#34;&gt;Postdoctoral Fellowship on NLP for Computational Social Science&lt;/h2&gt;

&lt;p&gt;The Pioneer Centre for AI and Department of Computer Science at the University of Copenhagen invite applications for a 2-year postdoctoral full-time research position in the domain of Natural Language Processing.&lt;/p&gt;

&lt;p&gt;NLP is becoming an increasingly powerful tool for social scientists. Yet, the intersection between the two disciplines is still poorly explored, with research in the two disciplines often being conducted as separate streams. The goal of this project is to research methods which can more directly be useful for downstream social science applications. One such application is to analyse common narratives in news, which requires methods including (interpretable) topic modelling, framing detection, social media analysis, etc. The successful candidate will be affiliated with a larger initiative on narrative analysis, spanning different content modalities, with the autonomy to define their project in this larger context.&lt;/p&gt;

&lt;p&gt;The research will be conducted in collaboration with researchers at the Pioneer Centre for Artificial Intelligence&amp;rsquo;s Speech and Language Collaboratory, &lt;a href=&#34;https://www.copenlu.com/&#34;&gt;CopeNLU&lt;/a&gt; and the &lt;a href=&#34;https://www.belongielab.org/&#34;&gt;Belongie Lab&lt;/a&gt;. Inquiries about the position can be made to &lt;a href=&#34;mailto:augenstein@di.ku.dk&#34;&gt;Professor Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Application deadline: 7 April 2024. &lt;a href=&#34;https://jobportal.ku.dk/videnskabelige-stillinger/?show=161353&#34;&gt;Apply here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;postdoctoral-fellowship-on-multi-modal-fact-checking&#34;&gt;Postdoctoral Fellowship on Multi-Modal Fact Checking&lt;/h2&gt;

&lt;p&gt;The Pioneer Centre for AI and Department of Computer Science at the University of Copenhagen invite applications for a 2-year postdoctoral full-time research position in the domain of Natural Language Processing.&lt;/p&gt;

&lt;p&gt;Online content can include multiple different modalities, ranging from text to images or tables. Increasingly, detecting false information requires the understanding of a combination of these modalities and the relationship between them. This project will focus on developing general-purpose multi-modal methods for automatic fact checking in various domains, such as scientific publications, news or social media. Inquiries about the position can be made to &lt;a href=&#34;mailto:augenstein@di.ku.dk&#34;&gt;Professor Isabelle Augenstein&lt;/a&gt; or &lt;a href=&#34;mailto:de@di.ku.dk&#34;&gt;Assistant Professor Desmond Elliot&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Application deadline: 7 April 2024. &lt;a href=&#34;https://jobportal.ku.dk/videnskabelige-stillinger/?show=161352&#34;&gt;Apply here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Investigating Human Values in Online Communities</title>
      <link>https://copenlu.github.io/publication/2024_arxiv_borenstein/</link>
      <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_arxiv_borenstein/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Investigating the Impact of Model Instability on Explanations and Uncertainty</title>
      <link>https://copenlu.github.io/publication/2024_arxiv_marjanovic/</link>
      <pubDate>Tue, 20 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_arxiv_marjanovic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Understanding Fine-grained Distortions in Reports of Scientific Findings</title>
      <link>https://copenlu.github.io/publication/2024_arxiv_wuehrl/</link>
      <pubDate>Mon, 19 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_arxiv_wuehrl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models</title>
      <link>https://copenlu.github.io/publication/2024_eacl_arakelyan/</link>
      <pubDate>Thu, 01 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_eacl_arakelyan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>5 Papers Accepted to EMNLP 2023</title>
      <link>https://copenlu.github.io/talk/2023_12_emnlp/</link>
      <pubDate>Tue, 05 Dec 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2023_12_emnlp/</guid>
      <description>&lt;p&gt;5 papers by CopeNLU authors are accepted to appear at EMNLP 2023, on topics ranging from explainability to language modelling.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2023_emnlp_choudhury/&#34;&gt;Explaining Interactions Between Text Spans&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/sagnik-ray-choudhury/&#34;&gt;Sagnik Ray Choudhury&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/pepa-atanasova/&#34;&gt;Pepa Atanasova&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2023_emnlp_kaffee-wikipedia/&#34;&gt;Why Should This Article Be Deleted? Transparent Stance Detection in Multilingual Wikipedia Editor Discussions&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/lucie-aim%C3%A9e-kaffee/&#34;&gt;Lucie-Aimée Kaffee&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/arnav-arora&#34;&gt;Arnav Arora&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2023_emnlp_kaffee-dual/&#34;&gt;Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/lucie-aim%C3%A9e-kaffee/&#34;&gt;Lucie-Aimée Kaffee&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/arnav-arora&#34;&gt;Arnav Arora&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/zeerak-talat&#34;&gt;Zeerak Talat&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2023_emnlp_sen/&#34;&gt;People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/indira-sen/&#34;&gt;Indira Sen&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/dennis-assenmacher/&#34;&gt;Dennis Assenmacher&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/mattia-samory&#34;&gt;Mattia Samory&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/wil-van-der-aalst/&#34;&gt;Wil van der Aalst&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/claudia-wagner/&#34;&gt;Claudia Wagner&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2023_emnlp_borenstein/&#34;&gt;PHD: Pixel-Based Language Modeling of Historical Documents&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/nadav-borenstein/&#34;&gt;Nadav Borenstein&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/phillip-rust/&#34;&gt;Philipp Rust&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/desmond-elliott/&#34;&gt;Desmond Elliott&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Grammatical Gender&#39;s Influence on Distributional Semantics: A Causal Perspective</title>
      <link>https://copenlu.github.io/publication/2023_arxiv_stanczak/</link>
      <pubDate>Thu, 30 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_arxiv_stanczak/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Invisible Women in Digital Diplomacy: A Multidimensional Framework for Online Gender Bias Against Women Ambassadors Worldwide</title>
      <link>https://copenlu.github.io/publication/2023_arxiv_golovchenko/</link>
      <pubDate>Wed, 29 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_arxiv_golovchenko/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Factcheck-GPT: End-to-End Fine-Grained Document-Level Fact-Checking and Correction of LLM Output</title>
      <link>https://copenlu.github.io/publication/2023_arxiv_wang/</link>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_arxiv_wang/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Social Bias Probing: Fairness Benchmarking for Language Models</title>
      <link>https://copenlu.github.io/publication/2023_arxiv_manerba/</link>
      <pubDate>Wed, 15 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_arxiv_manerba/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quantifying Gender Bias Towards Politicians in Cross-Lingual Language Models</title>
      <link>https://copenlu.github.io/publication/2023_plosone_stanczak/</link>
      <pubDate>Fri, 10 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_plosone_stanczak/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PhD position available in context of ERC Starting Grant project ExplainYourself</title>
      <link>https://copenlu.github.io/talk/2023_11_erc/</link>
      <pubDate>Tue, 07 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2023_11_erc/</guid>
      <description>&lt;p&gt;A PhD fellowship on explainable natural language understanding is available in CopeNLU. The successful candidate will be supervised by &lt;a href=&#34;http://isabelleaugenstein.github.io/&#34;&gt;Isabelle Augenstein&lt;/a&gt; and &lt;a href=&#34;https://apepa.github.io/&#34;&gt;Pepa Atanasova&lt;/a&gt;. The positions are offered in the context of an &lt;a href=&#34;https://erc.europa.eu/apply-grant/starting-grant&#34;&gt;ERC Starting Grant&lt;/a&gt; on &amp;lsquo;&lt;a href=&#34;https://erc.europa.eu/news/erc-2021-starting-grants-results&#34;&gt;Explainable and Robust Automatic Fact Checking (ExplainYourself)&lt;/a&gt;&amp;rsquo;. ERC Starting Grant is a highly competitive funding program by the &lt;a href=&#34;https://erc.europa.eu/homepage&#34;&gt;European Research Council&lt;/a&gt; to support the most talented early-career scientists in Europe with funding for a period of 5 years for blue-skies research to build up or expand their research groups.&lt;/p&gt;

&lt;p&gt;ExplainYourself proposes to study explainable automatic fact checking, the task of automatically predicting the veracity of textual claims using machine learning (ML) methods, while also producing explanations about how the model arrived at the prediction. Automatic fact checking methods often use opaque deep neural network models, whose inner workings cannot easily be explained. Especially for complex tasks such as automatic fact checking, this hinders greater adoption, as it is unclear to users when the models&amp;rsquo; predictions can be trusted. Existing explainable ML methods partly overcome this by reducing the task of explanation generation to highlighting the right rationale. While a good first step, this does not fully explain how a ML model arrived at a prediction. For knowledge intensive natural language understanding (NLU) tasks such as fact checking, a ML model needs to learn complex relationships between the claim, multiple evidence documents, and common sense knowledge in addition to retrieving the right evidence. There is currently no explainability method that aims to illuminate this highly complex process. In addition, existing approaches are unable to produce diverse explanations, geared towards users with different information needs. ExplainYourself radically departs from existing work in proposing methods for explainable fact checking that more accurately reflect how fact checking models make decisions, and are useful to diverse groups of end users. It is expected that these innovations will apply to explanation generation for other knowledge-intensive NLU tasks, such as question answering or entity linking.&lt;/p&gt;

&lt;p&gt;In addition to the principle investigator, PhD students and postdocs, the project team will also include collaborators from CopeNLU as well as external collaborators. Two PhD students as well as a postdoc have already been recruited as a result of earlier calls, and the project officially &lt;a href=&#34;https://copenlu.github.io/talk/2023_09_explainyourself/&#34;&gt;kicked off in September 2023&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Read more about reasons to join us &lt;a href=&#34;https://copenlu.github.io/post/why-ucph/&#34;&gt;here&lt;/a&gt;. You can read more about the position and apply &lt;a href=&#34;https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&amp;departmentId=18970&amp;ProjectId=160498&amp;MediaId=5&amp;SkipAdvertisement=false&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>People Make Better Edits: Measuring the Efficacy of LLM-Generated Counterfactually Augmented Data for Harmful Language Detection</title>
      <link>https://copenlu.github.io/publication/2023_emnlp_sen/</link>
      <pubDate>Fri, 03 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_emnlp_sen/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PHD: Pixel-Based Language Modeling of Historical Documents</title>
      <link>https://copenlu.github.io/publication/2023_emnlp_borenstein/</link>
      <pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2023_emnlp_borenstein/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
