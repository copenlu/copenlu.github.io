<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>CopeNLU on CopeNLU</title>
    <link>https://copenlu.github.io/</link>
    <description>Recent content in CopeNLU on CopeNLU</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>From Internal Conflict to Contextual Adaptation of Language Models</title>
      <link>https://copenlu.github.io/publication/2024_arxiv_marjanovic/</link>
      <pubDate>Thu, 25 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_arxiv_marjanovic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Factuality Challenges in the Era of Large Language Models</title>
      <link>https://copenlu.github.io/publication/2024_nature_augenstein/</link>
      <pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_nature_augenstein/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Grammatical Gender&#39;s Influence on Distributional Semantics: A Causal Perspective</title>
      <link>https://copenlu.github.io/publication/2024_tacl_stanczak/</link>
      <pubDate>Wed, 10 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_tacl_stanczak/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Unified Framework for Input Feature Attribution Analysis</title>
      <link>https://copenlu.github.io/publication/2024_arxiv_sun/</link>
      <pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_arxiv_sun/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Measuring and Benchmarking Large Language Models&#39; Capabilities to Generate Persuasive Language</title>
      <link>https://copenlu.github.io/publication/2024_arxiv_pauli/</link>
      <pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_arxiv_pauli/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SynDARin: Synthesising Datasets for Automated Reasoning in Low-Resource Languages</title>
      <link>https://copenlu.github.io/publication/2024_arxiv_ghazaryan/</link>
      <pubDate>Fri, 21 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_arxiv_ghazaryan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Participate in research on explainable fact checking</title>
      <link>https://copenlu.github.io/talk/2024_05_interviews/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2024_05_interviews/</guid>
      <description>

&lt;p&gt;We are recruiting professional fact checkers to take part in an interview and/or a survey about their experiences of fact checking and fact checking technologies.&lt;/p&gt;

&lt;p&gt;If you are interested in participating in this research (interviews, surveys, or both), please complete the short online form linked below. A member of the research team will then contact you with more information about the study and taking part.&lt;/p&gt;

&lt;p&gt;Interview participants will be offered an &lt;a href=&#34;https://www.tangocard.com/reward-catalog?rewardcountries=US&amp;rewardcategory=gift+card&#34;&gt;online gift voucher&lt;/a&gt; to the value of 50 USD as compensation for their time. Participants who complete the survey will be offered an online gift voucher to the value of 15 USD. All personal data you may share will be kept confidential within the research team.&lt;/p&gt;

&lt;h2 id=&#34;p-style-text-align-center-a-href-https-forms-office-com-e-cwrtediqbf-origin-lprlink-sign-up-form-a-p&#34;&gt;&lt;p style=&#34;text-align: center;&#34;&gt;&lt;a href=&#34;https://forms.office.com/e/CwrTediqbF?origin=lprLink&#34;&gt;Sign up form&lt;/a&gt;&lt;/p&gt;&lt;/h2&gt;

&lt;h2 id=&#34;what-s-involved&#34;&gt;Whatâ€™s involved?&lt;/h2&gt;

&lt;p&gt;We are conducting remote interviews (e.g., on Zoom) and online surveys with professional fact-checkers, members of the general public, and other stakeholders in the fact checking sector such as journalists and content moderators.&lt;/p&gt;

&lt;p&gt;The interviews will be 60 minutes in duration and will take place remotely via Zoom. Interview participants will be offered an online gift voucher to the value of 50 USD as compensation for their time.&lt;/p&gt;

&lt;p&gt;Participants who agree to take part in the survey will receive a link via email to an online survey in July 2024. The survey will take about 20 minutes to complete. All participants who complete the survey will be offered an online gift voucher to the value of 15 USD.&lt;/p&gt;

&lt;p&gt;All personal data will be kept confidential within the project team and always anonymised for publications and presentations. This project has received ethical approval from the University of Copenhagen Research Ethics Committee for the Faculty of Science and Faculty of Health and Medical Sciences.&lt;/p&gt;

&lt;p&gt;You can read the
&lt;a href=&#34;https://www.copenlu.com/talk/2024_05_interviews/InformationSheet_FactChecking.pdf&#34;&gt;study information sheet here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-take-part&#34;&gt;How do I take part?&lt;/h2&gt;

&lt;p&gt;Simply fill in &lt;a href=&#34;https://forms.office.com/e/CwrTediqbF?origin=lprLink&#34;&gt;the online form here&lt;/a&gt;, and we will get in touch with you with more information.
If you have any questions or would like more information about the project, you can contact Greta Warren at &lt;a href=&#34;mailto:grwa@di.ku.dk&#34;&gt;grwa@di.ku.dk&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-is-this-research-about&#34;&gt;What is this research about?&lt;/h2&gt;

&lt;p&gt;This research is part of the European Research Council-funded &lt;a href=&#34;https://cordis.europa.eu/project/id/101077481&#34;&gt;ExplainYourself project&lt;/a&gt; (grant agreement ID no. 101077481), which focuses on explainable automatic fact checking.  Explainable automatic fact checking involves developing Artificial Intelligence (AI) systems that can detect and correct false information as well as produce explanations about how a system arrived at its prediction that a particular piece of information is true or false.&lt;/p&gt;

&lt;p&gt;The aim of the current research is to understand what kinds of explanations people require when using or when impacted by automated fact checking systems, and how these information needs may differ between different groups of stakeholders. We seek to ensure that the explanations that these systems provide are truly useful to the people that interact with them.&lt;/p&gt;

&lt;h2 id=&#34;research-team&#34;&gt;Research Team:&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://gretawarren.github.io/&#34;&gt;Dr Greta Warren&lt;/a&gt; &lt;br /&gt;
&lt;em&gt;Postdoctoral Researcher, Department of Computer Science, University of Copenhagen&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://miswritings.org/&#34;&gt;Prof. Irina Shklovski&lt;/a&gt; &lt;br /&gt;
&lt;em&gt;Professor, Department of Computer Science, University of Copenhagen&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://isabelleaugenstein.github.io/&#34;&gt;Prof. Isabelle Augenstein&lt;/a&gt; (Principal Investigator)&lt;br /&gt;
&lt;em&gt;Professor, Department of Computer Science, University of Copenhagen&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods</title>
      <link>https://copenlu.github.io/publication/2024_acl_yu/</link>
      <pubDate>Wed, 29 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_acl_yu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What Languages are Easy to Language-Model? A Perspective from Learning Probabilistic Regular Languages</title>
      <link>https://copenlu.github.io/publication/2024_acl_borenstein/</link>
      <pubDate>Tue, 28 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_acl_borenstein/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Investigating the Impact of Model Instability on Explanations and Uncertainty</title>
      <link>https://copenlu.github.io/publication/2024_acl_marjanovic/</link>
      <pubDate>Mon, 20 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_acl_marjanovic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Understanding Fine-grained Distortions in Reports of Scientific Findings</title>
      <link>https://copenlu.github.io/publication/2024_acl_wuehrl/</link>
      <pubDate>Sun, 19 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_acl_wuehrl/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Oustanding paper award at EACL 2024</title>
      <link>https://copenlu.github.io/talk/2024_03_eacl/</link>
      <pubDate>Wed, 20 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2024_03_eacl/</guid>
      <description>&lt;p&gt;We are honoured to share that our paper on measuring the fragility of natural language inference models has won an outstanding paper award at EACL 2024. The paper is based on the MSc thesis of Zhaoqi Liu, who was supervised by Isabelle Augenstein and Erik Arakelyan.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://copenlu.github.io/publication/2024_eacl_arakelyan/&#34;&gt;Semantic Sensitivities and Inconsistent Predictions: Measuring the Fragility of NLI Models&lt;/a&gt;.
&lt;a href=&#34;https://copenlu.github.io/authors/erik-arakelyan/&#34;&gt;Erik Arakelyan&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/zhaoqi liu/&#34;&gt;Zhaoqi Liu&lt;/a&gt;, &lt;a href=&#34;https://copenlu.github.io/authors/isabelle-augenstein/&#34;&gt;Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;



&lt;div class=&#34;gallery&#34;&gt;

  
  
  
  
    
    
    
    
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://copenlu.github.io/talk/2024_03_eacl/gallery/award.jpg&#34; &gt;
  &lt;img src=&#34;https://copenlu.github.io/talk/2024_03_eacl/gallery/award.jpg&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  
    
    
    
    
    
      
    
      
    
    
  &lt;a data-fancybox=&#34;gallery-gallery&#34; href=&#34;https://copenlu.github.io/talk/2024_03_eacl/gallery/award2.jpg&#34; &gt;
  &lt;img src=&#34;https://copenlu.github.io/talk/2024_03_eacl/gallery/award2.jpg&#34; alt=&#34;&#34;&gt;
  &lt;/a&gt;
  

  
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The LEADING Guideline Reporting Standards for Expert Panel, Best-Estimate Diagnosis, and Longitudinal Expert All Data (LEAD) Studies</title>
      <link>https://copenlu.github.io/publication/2024_arxiv_eijsbroek/</link>
      <pubDate>Tue, 19 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_arxiv_eijsbroek/</guid>
      <description></description>
    </item>
    
    <item>
      <title>PhD and postdoc positions available at Pioneer Centre for AI</title>
      <link>https://copenlu.github.io/talk/2024_03_positions/</link>
      <pubDate>Fri, 01 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/talk/2024_03_positions/</guid>
      <description>

&lt;p&gt;A PhD and two postdoc positions on natural language understanding are available. The positions are funded by the &lt;a href=&#34;https://www.aicentre.dk/&#34;&gt;Pioneer Centre for AI&lt;/a&gt;. Read more about reasons to join us &lt;a href=&#34;https://copenlu.github.io/post/why-ucph/&#34;&gt;here&lt;/a&gt;. You can read more about the positions at the Pioneer Centre &lt;a href=&#34;https://www.aicentre.dk/jobs&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;phd-fellowship-on-factual-text-generation&#34;&gt;PhD Fellowship on Factual Text Generation&lt;/h2&gt;

&lt;p&gt;While recent large language models demonstrate surprising fluency and predictive capabilities in their generated text, they have been demonstrated to generate factual inaccuracies even when they have encoded truthful information. This limits their utility and safety in real world scenarios where guarantees of factuality are needed. To address this, the project will explore methods for improving the factuality of text generation with respect to both objective real-world facts and provided source documents.&lt;/p&gt;

&lt;p&gt;We are looking for candidates with a background in computer science, machine learning, natural language processing, computational social science, or similar. The candidate should have an interest in automatic text generation and fact checking. They should also have an interest in interdisciplinary research endeavors, including at the Pioneer Center for AI. Early research experience, especially with empirical research methods, or relevant industry experience, will be a bonus.&lt;/p&gt;

&lt;p&gt;The principal supervisor is &lt;a href=&#34;mailto:augenstein@di.ku.dk&#34;&gt;Professor Isabelle Augenstein&lt;/a&gt; and the co-supervisor is &lt;a href=&#34;mailto:dw@di.ku.dk&#34;&gt;Dustin Wright&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Application deadline: 1 April 2024. &lt;a href=&#34;https://candidate.hr-manager.net/ApplicationInit.aspx/?cid=1307&amp;departmentId=18970&amp;ProjectId=161235&amp;MediaId=5&amp;SkipAdvertisement=false&#34;&gt;Apply here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;postdoctoral-fellowship-on-nlp-for-computational-social-science&#34;&gt;Postdoctoral Fellowship on NLP for Computational Social Science&lt;/h2&gt;

&lt;p&gt;The Pioneer Centre for AI and Department of Computer Science at the University of Copenhagen invite applications for a 2-year postdoctoral full-time research position in the domain of Natural Language Processing.&lt;/p&gt;

&lt;p&gt;NLP is becoming an increasingly powerful tool for social scientists. Yet, the intersection between the two disciplines is still poorly explored, with research in the two disciplines often being conducted as separate streams. The goal of this project is to research methods which can more directly be useful for downstream social science applications. One such application is to analyse common narratives in news, which requires methods including (interpretable) topic modelling, framing detection, social media analysis, etc. The successful candidate will be affiliated with a larger initiative on narrative analysis, spanning different content modalities, with the autonomy to define their project in this larger context.&lt;/p&gt;

&lt;p&gt;The research will be conducted in collaboration with researchers at the Pioneer Centre for Artificial Intelligence&amp;rsquo;s Speech and Language Collaboratory, &lt;a href=&#34;https://www.copenlu.com/&#34;&gt;CopeNLU&lt;/a&gt; and the &lt;a href=&#34;https://www.belongielab.org/&#34;&gt;Belongie Lab&lt;/a&gt;. Inquiries about the position can be made to &lt;a href=&#34;mailto:augenstein@di.ku.dk&#34;&gt;Professor Isabelle Augenstein&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Application deadline: 7 April 2024. &lt;a href=&#34;https://jobportal.ku.dk/videnskabelige-stillinger/?show=161353&#34;&gt;Apply here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;postdoctoral-fellowship-on-multi-modal-fact-checking&#34;&gt;Postdoctoral Fellowship on Multi-Modal Fact Checking&lt;/h2&gt;

&lt;p&gt;The Pioneer Centre for AI and Department of Computer Science at the University of Copenhagen invite applications for a 2-year postdoctoral full-time research position in the domain of Natural Language Processing.&lt;/p&gt;

&lt;p&gt;Online content can include multiple different modalities, ranging from text to images or tables. Increasingly, detecting false information requires the understanding of a combination of these modalities and the relationship between them. This project will focus on developing general-purpose multi-modal methods for automatic fact checking in various domains, such as scientific publications, news or social media. Inquiries about the position can be made to &lt;a href=&#34;mailto:augenstein@di.ku.dk&#34;&gt;Professor Isabelle Augenstein&lt;/a&gt; or &lt;a href=&#34;mailto:de@di.ku.dk&#34;&gt;Assistant Professor Desmond Elliot&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Application deadline: 7 April 2024. &lt;a href=&#34;https://jobportal.ku.dk/videnskabelige-stillinger/?show=161352&#34;&gt;Apply here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Investigating Human Values in Online Communities</title>
      <link>https://copenlu.github.io/publication/2024_arxiv_borenstein/</link>
      <pubDate>Wed, 21 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://copenlu.github.io/publication/2024_arxiv_borenstein/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
